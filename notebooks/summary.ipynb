{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have the following packages:\n",
    "\n",
    "pip install youtube-transcript-api\n",
    "pip install pytube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "# get OpenAI key env\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"hey everyone and welcome to this session about Azure container apps so a session uh not about kubernetes and maybe even a way to say goodbye kubernetes for some applications we'll see how far we get now my name is I work for the Chronos group as a cloud native architect I'm also a Microsoft Azure MVP you can contact me via Twitter or via my YouTube channel where you'll find similar videos similar to this presentation and there's actually a video one of the latest ones which is about the sample application I'm going to use in this session uh it's called open AI to e generator on Azure container app so feel free to look at that for some more details now there are also some resources available everything I'm talking about is available on GitHub so in the ACA open AI repository and you'll also find a site just called aca.super.site that one creates or has another demo that goes into some more detail deploying different container apps to a single container absent environment so it's a different demo and the one I'm using here it's a bit more elaborate so we feel free to look at that one as well now I'm not going to talk about the advantages of containers or why you should actually develop in containers and what's the effect on developers and so on we know this right I believe that you are convinced that that is the way to go but then you have of course the need to deploy these containers to yeah let's say a service that can handle that and depending on the cloud vendor or on premises there are lots of different ways to run your containers when you look at the Azure Cloud if you're looking on a range from simple to more complex there are several services that can run your containers the first one is container instances Microsoft first attempt at containers as a service it is a rather bare bone service in the sense that it doesn't have some of the higher level features like TLS termination custom certificates scaling and so on but it's quite good at let's say throwing a container at it doing some let's say more complex tasks running it only paying for the seconds it is running and then actually stopping that container and it supports gpus and as well so it can be used for workloads that require gpus and it supports Linux and windows containers now normally for let's say customer facing applications especially web apps and web apis we would not recommend to use the the more Bare Bones container instances we would recommend first of all to look at app Services app Services has existed for quite a long time it is compatible to things like Google app engine and elastic Beanstalk and it doesn't even need containers you can just have your code the code can be thrown at app service so to speak and it supports several runtimes like node and Python and of course also.net um that already exists for quite some time and later running containers on top of app Services was added as well app services does have higher level features like for example TLS termination custom certificates Azure key Vault integration for your secrets it has a good scaling story as well there are lots of different features available now if your applications are somewhat more complex or you have a lot of background workers to run then normally we wouldn't recommend app Services we would recommend to at least in the past to use something like kubernetes of course there's more complexity there but you have much more control over things like what kind of Hardware are we using do we need gpus or not are we going to use Linux or Windows containers uh do we have a complex microservices application we need service meshes maybe other kinds of tooling that exists in in the market kubernetes is extremely flexible of course at the expense of some more complexity now what Microsoft has done is create a newer service which is called container apps and that's actually in the middle between app services and kubernetes services so it can run the workloads that app Services can run quite easily but it's also optimized for creating more complex applications and micro Services applications in fact if you look at container apps a bit more from a bit from a bit closer you see it's actually a serverless containers as a service solution optimized for microservices containers as a service means you can focus on the app not the infrastructure if you have to do kubernetes you know you still have to think about kubernetes upgrades security of your nodes security patching maybe replacing the node image once in a while and you have to also think more about the networking and security story what about Ingress how to do Ingress and so on so there is much more Focus still on infrastructure with something like kubernetes not so with something like container apps it is serverless in the sense that it can scale dynamically based on external events can be HTTP events but also other events like messages in a bus for example or in a queue it's also serverless in the sense that it can scale to zero when there is no work to be done then there will know then there will be no containers running that means you also don't pay for your containers now serverless also means that you are going to be billed by the second and for memory and CPU consumed so it's a factory a serverless platform very similar in that sense to something like for example Google Cloud functions which is also a serverless containers as a service solution now Microsoft could have built this on their own but they decided to build this on open source and actually kubernetes is under the hood all your container apps when you run them they actually run any kubernetes namespace but you don't see that unless you delve quite deep and you don't have access to the kubernetes API server you always have to use the simpler container apps apis to actually work with kubernetes underneath basically you don't have to care about kubernetes that's the main point here is used for this Dynamic scaling based on events geta is the kubernetes event Driven Auto scaler that you can also install on your own clusters it's not unique to container apps but it comes in the box is installed for you and updated for you the same thing goes for Dapper the distributed application runtime basically a toolbox for developers to make creating distributed applications easier well Dapper again comes in the box is updated for you and you use it if you want to create microservices applications it's optional though and for networking Microsoft is using Envoy it's used for external access or public access to your containers so for Ingress it's also used for internally making your applications available in the in the internal container environment as you'll see later now container apps of course is a service it's a past service it's a platform as a service solution that also means that there are limitations right that are not currently enabled on the service a couple of things here that might be important is that only Linux containers are supported you can't use any hardware underlying Hardware you want that's not selectable you can't use Hardware with gpus or use gpus entirely that's not possible you cannot for example use Arm servers that you cannot do and containers are limited to two CPU cores and four gigs of RAM so for many of these limitations of course there's an alternative solution that solution would be to run that workload on kubernetes which of course is flexible enough to do all those kinds of things so it's always an evaluation you have to do right so when you have an application look at what the application needs and then look can we run this on container apps is that sufficient and if not of course use something else so it's not let's say 100 goodbye kubernetes from now on it's like with any service you have to look at your application and then select the application or the service that is good for your application now as a reminder what you can build on top of app Services sorry container apps uh the following applications first of all apis and web apps with complete automatic scaling built in so by default even scaling is configured between zero and I think even 10 instances and you have to control the way you want to do scaling but there's no configuration to do if you want to Scale based on concurrent HTTP request you just set the minimum and the maximum and that's it for these kind of applications you can also split your traffic Envoy is used to do that for example if you have a new version we call it revisions here and you want to send 10 of traffic to the new version and 90 to the old version for for example a canary release well you can do so with the container apps apis quite easily container apps can also be used for background processing for example you have a content has to continuously run as a background process no problem you can scale that container based on CPU and memory consumed in that case cada has to be configured to configure this scaling it can also be used for event processing for example you need to read messages from a topic or EQ and you want to automatically Scale based on the number of messages in that queue you can configure a cada scanner to do so that queue can be in any Cloud most kind of queues and clouds are supported so an Azure that would typically be Azure service bus but there's also support for things like rabbit mq and Kafka and so on now you might think well don't we have something like serverless platforms like Lambda and Google Cloud functions and Azure functions to do this yes but in most cases with those kind of solutions you have to follow a specific programming model in this case you design you write your code the way you want to package it in a container and use that for your event processing I sometimes hear the term using things like Azure functions for lightweight event processing and then this for more like heavy duty event processing well I'm not going to make a decision on that but that's sometimes how it is described and then of course you have microservices as explained container apps camera and microservices with optional Dapper integration there and these microservices within an environment I will talk about an environment later they can easily talk to each other over the internal Network that that's of course needed when they do synchronous communication or you use asynchronous communication of course with a service or something like that that depends of course on your application architecture so a whole wide range of applications that you can write on top of container apps but in the past we would say well this type of app maybe you can use app services for this one you have to use kubernetes now you actually have a choice that can do all of these quite well and then with exceptions maybe you would go to kubernetes depending on your application needs now to run a container app right to run your containers you have to have a container apps environment such an environment is easily created takes a few minutes to create and then you get access to this AKs under the hood but again remember you don't have access to the kubernetes apis it is just there for you you don't have to worry about it this environment is accent is essentially free the only thing which is not free is if you select log analytics to keep all your system and container logs historically that of course consume storage and there's a price to pay for keeping those logs historically there's an option to also not use log analytics in a development environment for example to only see the current logs that's great indeed for development but for production you would normally enable log analytics to keep all your logs so there's no need to configure any kind of logging tool or something like that for this environment it's it's built in with local analytics integration by default such an environment gets a public IP and all the containers that have an Ingress they will get a automatic DNS name that links to that public IP now of course for most customers they need to make these things available also on the internal Network you can optionally link a container apps environment at creation time that's important that creation time not afterwards link it to a virtual Network and if that virtual network is linked to let's say your on-prem network using a VPN then you can connect to these container apps from the internal Network now I'm going to deploy later an application with two apps I will have a UI and an API well when we deploy the API sorry the UI for the first time you will have container app one my first container app and directly I will get my first revision running on my first version of my application a revision runs replicas zero or more replicas depending on the scaling rules that you configured and you can compare a replica with a port on kubernetes so if you know something about pods you know you can run multiple containers concurrently like your main container and sidecar containers you can do so in container apps as well you can have things like init containers configured you can have things like for example probing like a liveness probe or a Readiness probe all these Concepts that you know from kubernetes are actually in container apps as well you just configure them using the container apps API and under the hood kubernetes comes structs are used another example for example would be to set a secret for a container app a secret would actually be translated under the hood to a kubernetes secret now when I have my first version of my first revision and I deploy a new version of my application by default that second provision that new version will become the active one and the old one will be deactivated and archived and the system keeps like hundreds of these past revisions you can always reactivate them again later the default revision mode is single that means that the most recent deployed revision is the active one and receives all the traffic but remember when you do traffic splitting you can actually have multiple revisions active and send a percentage of traffic between your different revisions right note in that case that the revision scale independently so if you have scale rules for your revisions depending on the traffic ending up in each revision each revision will scale independently from the other and of course I will have my API in the back end that I call we'll see later what it does well of course I will have a second container app in the same environment and in the over the internal Network these two can communicate with each other a container apps environment is basically a security boundary I cannot really configure things like Network policies and so on to separate my containers within an environment if you need to separate containers from one another you would use the different container apps environments be aware that this container apps and environments have limits as well for example there's a limit of having 40 CPU cores active within an environment so if you have lots of container apps with lots of revisions with lots of scaling rules yeah the scaling is never guaranteed because if you are reaching the limit of 40 cores per environment of course you can't scale beyond that so you have to a bit watch out what you do there if you have scaling limits that need to go beyond what an environment can do of course there's still kubernetes available you can go to even like 5 000 nodes today I think in Azure so scaling in that sense is not really an issue there good now now we have to think about how we deploy to a container apps environment and without going into details about how we actually deploy the environment itself what about developer productivity and we know it is very difficult if you want that a developer deploy to a real kubernetes cluster um letting them do that all by themselves is usually not a very good idea it would you would provide such environments to them but with container apps if a developer has access to a resource Group in a let's say developer subscription a developer when working on source code can actually create a container app quite easily from the local machine with the Azure CLI and here you see the Azure container app up command where the focus is or should be on the dash dash Source option that actually says hey look at my source files in the current directory and if there's a Docker file in there please create a Docker image a go to Docker image to a container registry create a container app environment for me and then create a container all with one command and here you even make that container available externally on the internet so the Ingress is configured automatically as well right here we do some other things we say we have an existing environment already and so on and so on but even if you don't give it the environment option there it will create an environment for you automatically so it's really easy for developers to get their applications up and running quickly in the actual environment they are going to run it for them to test for example things like managed identity and stuff like that so there's a big focus on this let's say inner loop this developer productivity aspect but of course in a production environment you're going to automate the deployment of of these systems for example you would use normally you would use bicep that's what we recommend for our customers arm of course is still supported and if you're a terraform user no problem I think since February this year there's food support for Azure container apps in the Azure RM provider and if you're so inclined you can also use the Azure CLI or maybe even Powershell to deploy these these things for you um now bicep what would that mean to create an environment what would you have to do well actually just this this is enough to create an environment and Link it to a existing log analytics workspace if you compare this to the bicep to deploy a kubernetes cluster there there's much more thing to think about how many nodes how many node pools um there are lots of things you have to configure in this case yeah this just gives us access to an underlying kubernetes cluster you don't see so the environment itself is a fairly simple thing so this is all to deploy an environment now to deploy containers you can also use bicep you don't have to know things like a kubernetes gamma or use things like Helm charts and so on this is just plain bicep where we deploy a container app that we're gonna call something web UI and what you see here there's things like we configure a identity we set all kinds of properties for Ingress and in the end we have a template a template just like a pod template in a deployment where we specify the containers we want to run this is highly comparable to that so here we say this image or use this image and set some environment variables there are other things we can set here for example we can set things like the liveness probe the Readiness probe they can be set in this template as well and a bit higher if we go back a bit there you could also for example specify secrets and so on for our container so everything that I'm going to show you in a moment in the UI that will be that will be available in those available in bicep now the application we're going to create is a tweet generator so this is the UI which is very simple and ugly I'm not a graphical designer here you give it some text and a sentiment and it generates a well hopefully funny in this case tweet about kubernetes the Tweet is generated by an open AI model by the text davinci003 model and it's my API that does this for us the actual architecture of this application is here so we have our container app environment novnet so we get a public IP and when we make a container app in our case the web UI available on the internet via its automatic Ingress we can reach that web UI quite easily the web UI is also secured with Azure active directory I will show you in a moment how that is done and it's also enabled for Dapper because I'm going to use Dapper actually the Dapper invoke API only that which is a very small part of dapper I'm going to use Dapper to call into my API in the back end and to find the API easily on the network as you can see here on the slide as well there's a managed identity connected to the container apps if you have ever played with kubernetes and trying to give kubernetes a managed identity and then in Azure I'm not talking about other clouds here there's quite some work to do and sometimes even it's confusing like you had the pullback entity project but that's now you shouldn't use it anymore and I should actually use a workload identity which uses Federation there's quite some things to set up for that before that works a lot of yaml to configure in this case I'm just saying hey my web app here should have a management entity and it's done it's a few lines of code in bicep and it's done we are using this manage identity to actually authenticate to Azure container registry because that's a private registry and we have to say who we are before we can pull the image from that container registry the API is also using a managed identity to retrieve the Azure open AI secret from keyboard container apps today has no native integration with key Vault so if you want to retrieve things from keyboard your code has to do that at runtime and the API within a python is doing that using the Microsoft identity libraries and the key Vault libraries is the API itself is not really doing anything it's on its own it's going to call into an Azure open AI model to call a model which I call Twitter but it actually is configured to use the text 7g003 model their most the most powerful model today in Azure open AI service but that's not really important for this for this presentation so let's see what it looks like in the portal so here we are at the container apps environments in my subscription and the one I deployed with bicep what you've seen earlier the the simple bicep code a few lines of code that is this environment now in an environment there's not that much to do we can have something set at the environment level like certificates you want to use when we want to use custom names for our container apps or even things like a custom DNS suffix for all my container apps but that's in preview today what we also did with deployment is we configured a log analytics workspace you would see that in the login options and you can clearly see to which workspace we are sending our logs to see the historical logs we can just go to the logs section here and if you have ever worked with log analytics you know you can use certain queries here kql queries to basically see what's happening for example if I want to see my container apps on Zone logs and I want to find everything that contains AI API I can just run this and I see all the historical logs that were kept that contain AI API here and of course we have to drill down further to find what we want if I'm only interested in the current law stream the current loss or the most recent logs I can look at them as well without having to create a query now I am more interested in the apps section our container apps are listed here and we start with the web UI that gives us the UI to indeed get our tweet generated and an important aspect here is of course the Ingress how do we configure external access to this container app well the Ingress configuration in container apps compared to kubernetes is of course very simple we simply enable Ingress and we say we accept traffic from anywhere other options are to limit traffic to the environment only that would be the setting for my API and if we deploy a container app environment linked to a v-net we can actually make this container app environment only available internally these kinds of ingresses with external environments they only support HTTP so we can only access our application on ports 443 or 80. no other ports are supported and it's important to know because this is essentially a reverse proxy and Envoy is used for this reverse proxying Envoy of course needs to know the port that my container is using to accept HTTP requests and in my case that's 5000. now what I've also done is I've also configured authentication for this application it is a built-in feature into container apps that allows you to put an identity provider in front of your app it's basically a piece of middleware that does this authentication for you automatically you choose as a developer if you want to use this or you roll your own integration in your application that's basically up to you in my case I have to go to this app using a certain URL so if I go to the overview the Ingress that I've just shown you that results in an application URL which is this one over here when I click on this I'm getting the following UI and we'll see what happens and it stays black for a moment and the reason why it stays black is because this application is scaling to zero so here you see the effect of the let's say cold startup of my container app before it can respond with something and in my case now I see my application I can type in what I want for example kubernetes and let's make it a serious tweet this time and see if my application works and indeed here's my serious tweet about Google that is now this works now you haven't seen the authentication because already I had my cookie already configured I was already authenticated and if you would go to that to that application URL you would get a Azure active that actually log in to go into the app now I also configured my application to use Dapper I talked about that Dapper is enabled for my application I have to give my application a ID that is web UI here now I'm going to call my API using Dapper the what what does that mean well that means I have to do an HTTP call well in this case to that other container and I have to find it on the network how is that done well what I'm using here is something I've set as an environment variable which is the URL that we're going to use Dapper with and this is the invoke URL basically what you have to think about is Dapper is a sidecar in your replica so the sidecarbons next to your application container sidecars are addressable over localhost the Dapper sidecar runs on Port 3500 because Dapper has an HTTP API we use the invoke API to call a method called generate on a Dapper application adapter ID which is open Ai and if you look into my applications and you go to my API here you'll see that my Dapper is enabled on this API and that the Dapper ID given is indeed open AI so this name can be used by the web UI to easily translate this into a IP address so that can find the API on the network and then send the HTTP call the generate method basically can be called on this API so yes we have Dapper enabled on both applications enabling Dapper to be used to do HTTP calls with the invoke API there let's go back to the web UI because the web UI also has a identity configured this identity was configured with a couple of lines in bicep and we assigned a user assigned identity that basically means there's an Azure active directory identity that can be granted specific access rights that are application needs I'm now in that identity in Azure and I can check the role assignments I've given these assignments using bicep and you can see that this identity has the right ACN pull role to pull from our container registry this one over here and we've also given it the key Vault Secrets user role on this key Vault that's used by the API to retrieve the secret yes I've given the UI and the API the same identity I don't have to do that I could give them multiple or different identities there let's go back to my web UI and let's take a look at revision management because that's of course where all the magic happens when I deploy my container app I'm getting a revision I'm now in single revision mode which means that only the latest revision is active and receiving 100 of traffic if you go to that revision you'll see that there's some details here and there's also an option to let's say restart your revision it's a bit similar to restarting your kubernetes deployment really with the cube CTL tool that's what's happening in in the background yeah sometimes this restart is required for example you have probably seen that there are secrets here Secrets allow us to Define Secrets at a container apps level which actually are secrets at the kubernetes level but if these secrets change the containers don't pick them up automatically so a restart would be needed to pick up a change in the value of our secret and by the way to day these secrets you have to set the value here directly you can't tell container apps to grab the value of the secret from a keyboard so this keyboard integration is what I talked about earlier that's not available today that is on the roadmap and coming now my revision is now running I will of course have certain things configured in this revision so I can see the containers in this revision that's the web UI and this is coming from Azure container registry using a GitHub shower a gitsha as the image tag here my container has environment variables the invoke URL we talked about and I didn't configure Health probes here but if you go to my application to the API here in the API I did configure some help probes just as an example and you'll see that this is exactly what you expect from kubernetes Health probes this is a HTTP get liveness probe to a probe URL on Port 5001 in this case for this container and I'm having Readiness probe which is using the same the same port and so on to connect to the system and you're seeing here portal this would actually be not board which would be the URL order bad basically it's quite it's a small error in the in the UI here if I go back to my applications and I go back to my UI we also have our scaling and replicas here so with scale and replicas we can configure scaling for the app and notice that I told you this already that HTTP scaling is built in by default so if I set 0 2 that means scaling is active based on HTTP requests if I don't get any requests well in that case it will scale to zero the default is 10 concurrent HTTP requests but you can change this to another number if you want to by setting the scale rule in this session we're not looking at other kind of scale rules like kedar scale rules that Scale based on the number of messages in a queue for instance or a Kafka topic that's also possible with custom scale rules that you set if you go back a little bit below we see that all these container apps are integrated with Azure monitor so they all give you metrics that you can evaluate and you can also alert on so to see for example when did my when did a replica restart and so on and you can configure alerts off of this of course these containers also have their own lock stream now this is the current lock stream the real-time lock so the real-time logs of course won't be available if the app is not running so I have to quickly hit the app here and just open this in a new tab so I can actually get the application the container started again and up and running so I give it some time and I go back to the log stream I can see the current blocks for this container of course if I want to see the historical logs I will go to the logs option over here then as a last thing there's a console as well you can get so you can easily get a console to your running containers in this case we do that with shop and I can for example look at what's on my container and indeed there I have my app code running which is giving me the uh the front-end UI here so this is all the stuff that you can that you can configure we didn't look at all things for example things like custom domains are you good for example make a custom domain for this one and say I want this to be called tweet dot something else you can connect your own custom domain with your own certificate to this it's not automated so you would have to get a certificate and add it here there's not no automatic certificate integration like some other Azure Services actually have now if I go to the apps again and go to the API I have to show you one thing which is different in the API and that is of course the Ingress the API should not be available over the Internet the API should be limited to The Container apps and environment so Ingress has to be enabled if you don't enable Ingress you can't have traffic flowing to it but Ingress is enabled and only within the container apps environment you can actually reach this application similar as before only hdb and then of course the Target Port for our container what is it using to accept HTTP traffic for the rest all the rest is very very similar the the only difference here is that I'm never scaling to zero I'm leaving the API up and running and that's by setting Min and Max to one so it's always running so I can always go to my log stream it's always available and yeah because I have debug login here you can actually see the the full response okay I think this is enough for the for the demonstration so you have seen indeed this is what the deployment of an environment and container apps look like if you have ever seen kubernetes or kubernetes in Azure for example you can clearly see the difference that it's of course much simpler when it comes to configuring this and seeing what is going on in in the system sure there are some limitations but always evaluate your application and see is this application can it run in container apps without any issue I would recommend to use container apps instead of kubernetes your applications are more complex of course feel free to use kubernetes it's there for a reason so of course you use it when it is needed that's it thank you for listening and yeah hoping to see you another time bye bye\", metadata={'source': '-IJspV1HwGk'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.document_loaders import GoogleApiYoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=-IJspV1HwGk\")\n",
    "\n",
    "transcript = loader.load()\n",
    "\n",
    "# check the transcript\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This video discusses Azure Container Apps, a serverless container-as-a-service solution optimized for microservices. The presenter, a Cloud Native Architect, provides an overview of Azure Container Apps, its features, and how it compares to other container services like Kubernetes.\n",
       "\n",
       "The video is divided into the following sections:\n",
       "\n",
       "1. Introduction to Azure Container Apps: The presenter explains that Azure Container Apps is a serverless container-as-a-service solution built on Kubernetes, but without the complexity of managing Kubernetes infrastructure. It is designed for microservices and offers dynamic scaling based on events, billing by the second, and memory and CPU consumption.\n",
       "\n",
       "2. Features and Limitations: Azure Container Apps supports Linux containers, but has limitations such as no support for GPUs, ARM servers, and containers limited to 2 CPU cores and 4 GB of RAM. For more complex applications, Kubernetes may still be the better choice.\n",
       "\n",
       "3. Use Cases: Azure Container Apps can be used for web apps, APIs, background processing, event processing, and microservices. It offers built-in scaling, traffic splitting, and integration with Dapper and Envoy for networking.\n",
       "\n",
       "4. Deployment and Developer Productivity: The presenter demonstrates how to deploy a container app using the Azure CLI and bicep templates. He also shows how to configure ingress, authentication, and scaling for container apps.\n",
       "\n",
       "5. Sample Application: The presenter deploys a sample application consisting of a web UI and an API that generates tweets using OpenAI's text model. He demonstrates how to configure the environment, container apps, and revisions, as well as how to view logs and metrics.\n",
       "\n",
       "In summary, Azure Container Apps is a serverless container-as-a-service solution that simplifies the deployment and management of microservices applications. It offers many features and benefits over traditional Kubernetes deployments, but also has some limitations. Developers should evaluate their application requirements and choose the appropriate container service accordingly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "# use Azure chat model; requires gtp-4 deployment on given endpoint\n",
    "model = AzureChatOpenAI(\n",
    "    client=None,\n",
    "    openai_api_base=os.getenv(\"ENDPOINT\", \"\"),\n",
    "    openai_api_key=os.getenv(\"API_KEY\", \"\"),\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=os.getenv(\"DEPLOYMENT\", \"\"),\n",
    "    openai_api_type=\"azure\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "system_template = \"You are a helpful assistant that summarizes Youtube transcripts\"\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_template = \"Summarize the transcript of this video. If there are different sections, provide more details about each section: {transcript}\"\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=chat_prompt)\n",
    "\n",
    "result = chain.run(transcript=transcript)\n",
    "\n",
    "display(Markdown(result))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
